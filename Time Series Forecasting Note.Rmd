---
title: "Time Series Forecasting"
author: "Yinan Ding"
date: "2025-03-30"
output: html_document
---

```{r setup, include=FALSE} 
library(lubridate)
library(ggplot2)
library(forecast) #added for Acf and Pacf functions
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)
library(kableExtra)
library(readxl)
library(gridExtra)
library(cowplot)

getwd()

knitr::opts_chunk$set(collapse = TRUE)
```

# Time Series Introduction
Time Series is a set of observations on a variable collected over time.
Example: stock prices, interest rate, retail sales, electric power consumption, etc

Causes of variation of Time Series Data
- Seasons, holidays, etc
- Natural calamities: earthquake, epidemic, flood,drought, etc
- Political movements or changes, war, etc

An analysis of a single sequence of data is called *univariate* timeseries analysis.
An analysis of several sets of data for the same sequence of time periods is called *multivariate* time-series analysis

## Resources
Forecasting: Principles and Practice : https://otexts.com/fpp2/

Time Series Analysis with Applications - Cryer and Shan

https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/

## Main Concepts
- Mean: Average of a group of numbers
 $\mu_t = E(Y_t)$ 
- Variance: Average of squared differences from mean
$\sigma_t^2 = E[(Y_t - \mu_t)^2] = E(Y_t^2) - \mu_t^2$
- Std. Deviation: How spread out are the numbers

- Correlation: Measure linear dependence between two variables from the same time series

- Autocovariance: 
Describes how a time series covaries with itself at different lags. How a time series relates to its own past/future values.

$\gamma_{t,s} = \text{Cov}(Y_t, Y_s) = E[(Y_t - \mu_t)(Y_s - \mu_s)] = E[Y_t Y_s] - \mu_t \mu_s$

Autocorrelation: 
Standardize autocovariance by the variance to remove the scale dependence so it's easier for interpretation. Between -1 and 1.
$\rho_{t,s} = \text{Corr}(Y_t, Y_s) = \frac{\text{Cov}(Y_t, Y_s)}{\sqrt{\text{Var}(Y_t)\text{Var}(Y_s)}} = \frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t} \gamma_{s,s}}}$

$\gamma_{t,s}$ represents the variance at time t.

Autocovariance and Autocorrelation is used to detect patterns such as trends, seasonality, or cycles in data. Also used to choose models.

For example,  If the data shows a slow decay (ACF remains significant for many lags, e.g. >10), then likely a trend exists, the series is likely **non-stationary** (mean/variance change over time). The Solution will be to **Differencing** (compute first differences):

$\Delta Y_t = Y_t - Y_{t-1}$

### ACF & PACF: 
Measures the temporal dependency of a stochastic process and give us information about the **auto-regressive component** of the series.
Will always build the ACF and PACF before fitting a model to a stochastic process.

Autocorrelation Function (ACF) 
A stationary process Y_t at lag h, measures the linear dependency among the process variable Y_t and Y_t-h (correlation between a time series and it's lagged versions). Intermediate variables (Yt-1, Yt-2...Yt-h+1) plays a important role.

If ACF decays slowly: Suggests long-term dependence (common in AR models or non-stationary series).

If ACF cuts off sharply after lag q: Indicates an MA(q) process (since MA models have finite memory).

If ACF shows periodic spikes: Suggests seasonality.

- Partial Autocorrelation Function (PACF)
Consider PACF as ACF but **remove** the influence of all intermediate variables between Y_t and Y_t-h. It measures the direct correlation between Y_t and Y_t-h.

If PACF cuts off after lag p: Suggests an AR(p) model (since AR models depend only on the last p lags).

If PACF decays gradually: Suggests an MA or ARMA process (since MA models have infinite AR representations).

### Shocks
In time series modeling (AR, MA, ARIMA), a shock (also called an innovation or error term) refers to the unpredictable, random disturbance that affects the series at each time step. It represents the part of the series that cannot be explained by past values or trends.

Shocks $\epsilon_t$ are assumed to be independent and identically distributed (i.i.d.) with mean 0 and constant variance $\sigma^2$

Cannot be forecasted using past data. Example: Unexpected events like natural disasters, sudden market crashes, or measurement errors.

In AR models, shocks have a long-lasting effect (they influence future values recursively).

In MA models, shocks have a short-term effect (only affect the next q observations).



### Stationarity:

Definition
1. Mean function is constant for all time. $E(Y_t) = \mu$ for all t. 

2. Variance function is constant for all time, fluctuations are evenly distributed around the mean (No heteroskedasticity). $Var(Y_t)= \sigma^2$ for all t.

If the variance changes with time - you may be able to make it constant with log transformation

```{r visuals}
set.seed(123)
n <- 200
constant_var <- rnorm(n, mean = 0, sd = 1)  # Same variance throughout

# Plot
par(mfrow = c(1, 2))
plot.ts(constant_var, 
        main = "Constant Variance (Homoskedastic)",
        ylab = "Value",
        col = "blue")
abline(h = mean(constant_var), col = "red", lty = 2)

set.seed(123)
time <- 1:n
non_constant_var <- rnorm(n, mean = 0, sd = sqrt(time))  # Variance increases with time

# Plot
plot.ts(non_constant_var,
        main = "Non-Constant Variance (Heteroskedastic)",
        ylab = "Value",
        col = "darkorange")
abline(h = mean(non_constant_var), col = "red", lty = 2)

```


3. Autocovariance depends only on time difference and not the actual time.
$Cov(Y_t, Y_{t+k}) = \gamma_{t, t+k}$ for all t.

- Trend-Stationarity
1. Has a **deterministic trend** (can be modeled with a mathematical function such as linear, quadratic, exponential)
2. After subtracting the trend, the remaining series is stationary (constant mean, variance, and autocorrelation over time).
3. Forecasting using linear regression

A time series is **difference-stationary** if it becomes stationary after differencing. 
**Formula for First Difference:**  
$  \nabla Y_t = Y_t - Y_{t-1}  $  
**Generalized (d-th Difference):**  
$  \nabla^d Y_t = (1 - L)^d Y_t $  
where $L$ is the lag operator $(LY_t = Y_{t-1})$.


- Difference-Stationarity
1. Has a **stochastic trend** that follows a random walk with constant drift term (= unit root + constant trend)

2. Stochastic trend does not revert to a stable mean and exhibits permanent effects from past shocks. Require Differencing to convert to stationary.

3. Forecasting using ARIMA/SARIMA models

A series with a **stochastic trend** follows a unit root process:
$  Y_t = Y_{t-1} + \epsilon_t \quad \text{(Random Walk)}  $  
or with drift:  
$  Y_t = \alpha + Y_{t-1} + \epsilon_t \quad \text{(Random Walk with Drift)}  $  
where $ \epsilon_t \sim \text{WN}(0, \sigma^2)$ (white noise).

- Random walk: The series changes by random increments each period
- Drift term: Constant trend per period

- Unit Root
A unit root is a statistical property of a time series that makes it non-stationary because past shocks (random changes) have a permanent effect on future values. 

$Y_t = \theta Y_(t-1) + \epsilon_t $ (Standard Random Walk). 

$\epsilon_t$ = White Noise (random shock with mean 0 and variance $\sigma^2$)

Unit Root is a broader class of non-stationary time series where the coefficient of $Y_t-1$ in an autoregressive (AR) model is exactly 1 ($\theta = 1 $). If ($\theta < 1 $), then no unit root, stationary, shock fade/

Means: Never "forgets" past randomness (like a drunk person wandering unpredictably). Lacks a stable mean or variance over time.

With Unit Root (Problematic):
Every shock (e.g., a sudden economic event) permanently shifts the series.
Example: If GDP has a unit root, a recession today affects all future GDP values.

Without Unit Root (Good):
Shocks fade over time (mean-reversion).
Example: Temperature fluctuates but returns to a long-term average

Test for Stationarity 
1. Augmented Dickey-Fuller test - unit root
Tests the null hypothesis that the series is non-stationary (has a unit root).

adf.test(my_series)
p-value < 0.05 → Reject null (series is stationary).
p-value ≥ 0.05 → Fail to reject (series is non-stationary).

Note: ADF test can be misleading for seasonal data; use nsdiffs() and ndiffs() for better results.

in R use tseries::adf.test()

2. Phillips-Perron (PP) Test – unit root
3. Kitawoski-Phillips-Schmidt-Shin (KPSS) – unit root
4. Mann-Kendall Test  – monotonic trend
Common in detect deterministic trends in series of environmental data, climate data or hydrological data.
Cannot be applied to seasonal data

Null Hypothesis (H₀): No trend exists (data is independently ordered).
Alternative Hypothesis (H₁): A monotonic trend exists (either increasing or decreasing).

   - $\tau$ indicates trend strength and direction.
   - p-value < 0.05 indicates significant trend

in R - Kendall::MannKendall()

5. Spearman’s Rank Correlation Test  – monotonic trend
Spearman’s correlation coefficient is a statistical 
measure of the strength of a monotonic relationship

use stats::cor() or stats::cor.test(), if correlation is close to 0, then there's no trens

.... and more

```{r Stationarity Test and Differencing, include=FALSE}

#Augmented Dickey-Fuller test
tseries::adf.test(Y)

# Number of regular differences required
ndiffs(Y)  # Output: 1 (means difference once)

# Number of seasonal differences required
nsdiffs(Y) # Output: 1 (for seasonal data)

# First difference
diff_series <- diff(Y, differences = 1)
plot(diff_series, main = "First-Differenced Series")

# Seasonal difference (lag = 12 for monthly data)
seasonal_diff <- diff(Y, lag = 12)
plot(seasonal_diff, main = "Seasonally-Differenced Series")

# First difference + seasonal difference
combined_diff <- diff(diff(Y, differences = 1), lag = 12)
plot(combined_diff, main = "Trend- and Seasonally-Differenced Series")

adf.test(na.omit(combined_diff))

#Compare ACF/PACF Before vs. After Differencing
acf(Y, main = "ACF (Original)")
pacf(Y, main = "PACF (Original)")

acf(diff_series, main = "ACF (Regular Differenced)")
pacf(diff_series, main = "PACF (Regular Differenced)")

acf(combined_diff, main = "ACF (Differenced)")
pacf(combined_diff, main = "PACF (Differenced)")

# auto.arima
library(forecast)
auto_model <- auto.arima(Y)
summary(auto_model)
```


### Some Statistical Facts

1. Variance Equivalence  $\gamma_{t,t} = Cov(Y_t,Y_t)= Var(Y_t)$
The autocovariance at lag zero measures the covariance of Y_t with itself, which is the variance of Yt.

2. Self-Correlation $\rho_{t,t} = \frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t}\gamma_{s,s}}} = \frac{Var(Y_t)}{Var(Y_t)} = 1$
The autocorrelation at lag zero is the normalized version of the autocovariance of t and itself. 

3.Symmetry of Autocovariance $\gamma_{t,s} = \gamma_{s,t}$
Autocovariance is symmetric because covariance is commutative

4.Symmetry of Autocorrelation $\rho_{t,s} = \rho_{s,t}$
Autocorrelation inherits symmetry from autocovariance

5. Cauchy-Schwarz Inequality for Autocovariance  $|Cov(Y_t,Y_s)| <= \sqrt{Var(Y_t)Var(Y_s)} = |\gamma_{t,s}| <= \sqrt{\gamma_{t,t} \, \gamma_{s,s}}$
It ensures autocovariance is bounded by the product of standard deviations.

6. Bounds of Autocorrelation $|\rho_{t,s}| \leq 1$

```{r Examples for Above Concepts, include=FALSE}
#Visual Observation
Y<- daily_load_data$ave_daily_load
plot(Y, type = "l", main = "Time Series Plot")

# Calculate mean
mu <- mean(Y)
mu

# Calculate variance
sigma2 <- var(Y)
sigma2

# Calculate autocovariance and autocorrelation
acf_cov <- acf(Y, type = "covariance", plot = FALSE)

acf_cor <- acf(Y, type = "correlation", plot = FALSE)

# Display first few lags
head(acf_cov$acf) 
# [1,] Lag 0 (variance of Yt). The large magnitude suggests a high variability.
# [2,] Lag 1 (Cov(Y_t, Y_{t+1}))
# [3,] Lag 2 (Cov(Y_t, Y_{t+2}))
# [4,] Lag 3 (Cov(Y_t, Y_{t+3}))
# [5,] Lag 4 (Cov(Y_t, Y_{t+4}))
# [6,] Lag 5 (Cov(Y_t, Y_{t+5}))

head(acf_cor$acf)
# [1,] Lag 0, always 1, Corr(Y_t, Y_t)
# [2,] Lag 1 (Corr(Y_t, Y_{t+1}))
# [3,] Lag 2 (Corr(Y_t, Y_{t+2}))
# [4,] Lag 3 (Corr(Y_t, Y_{t+3}))
# [5,] Lag 4 (Corr(Y_t, Y_{t+4}))
# [6,] Lag 5 (Corr(Y_t, Y_{t+5}))

acf(Y, main = "Autocorrelation Function (ACF)")
#In the ACF plot: Blue dash line are confidence intervals (typically 95% confidence), ACF spike crossing these lines indicates significant autocorrelation at that lag. In this example, all lags up to Lag 30 exceed the blue lines. This suggests a very strong autocorrelation at all lags, therefore is a non-stationary series with a dominant trend. ACF decays slowly because past values remain correlated with future values due to the trend. Therefore will need to remove trend use differencing.

```


## Components of TSA 
### 1. Trend 
(General tendency to grow or decline over a long period, long-term tendency)

1.1 Linear Trend
Simple Linear Regression $Y_i = \beta_0 + \beta_1 t_i + \varepsilon_i$
Slope $\beta_0$ and the intercept $\beta_1 t_i$ are the unknown 
parameters (can be measured through the Least Squares Method), and $\varepsilon_i = Y_i - \hat Y_i$ is the error term.

Estimating Linear Trend using lm(Y~t)
- Y: Vector with observed series
- t: Vector from 1 to number of observations of Y

1.2 Non-Linear Trend
Trend can also be non-linear, such as the polynomial and exponential.

Moving Average can be used to estimate non-linear trend to smooth out short-term fluctuations and highlight longer-term trends or cycles.
Example: 3-day SMA of stock prices [10, 12, 11, 13] → SMA = (10+12+11)/3 = 11, (12+11+13)/3 = 12.

3 here is called Window Size (n)

### 2. Cycle 
(An up and down repetitive movement. Repeat itself over a long period of time)

### 3. Seasonal Variation 
(An up and down repetitive movement occurring periodically. Will see spikes at fixed intervals in ACF plot. Factor that cause seasonal variations: climate and weather
condition or custom traditions and habits

3.1 smoothing the trend with a moving average
3.2 De-trend the series
a. Addictive Model, detrend by take original series and subtract smoothed trend. *The magnitude of seasonality doesn't change in relation to time.
b. Multiplicative Model, detrend by taking original series and divide the original data by the trend.
3.3 Seasonal Means Model
 Assume the observed detrended series can be represented as  
$ Y_{seasonal,t} = \mu_t + X_t $ where \( E[X_t] = 0 \).  

- For monthly seasonal data, assume 12 parameters:  
$ \mu_t = 
\begin{cases} 
\beta_1 & \text{for } t = 1,13,25, \cdots \\ 
\beta_2 & \text{for } t = 2,14,26, \cdots \\ 
\vdots & \vdots \\ 
\beta_{12} & \text{for } t = 12,24,36, \cdots 
\end{cases} $  

3.4 Estimate the parameters $ \beta_1,\beta_2,...\beta_12 $
Create dummies (categorical variables with 2 levels)
$ D_{s,t} = 
\begin{cases} 
1 & \text{if } t \text{ belongs to season } s \\
0 & \text{otherwise}
\end{cases} $

for $ s = 1,2,\dots,12 $.

At any time period t, one of the seasonal dummies  
$D_{1,t}, D_{2,t},\dots, D_{12,t} $ will equal 1, all the others will equal 0.

3.5 Write series Y_seasonal as a function of dummies
$ Y_{seasonal,t} = \sum_{s=1}^{12} \beta_s D_{s,t} $
3.6 Compute coefficients by linear regression

In R:
dummies = forecast::seasonaldummy(Y)
Y need to be a time series object and Y=ts(Y, frequency =12)
Run a simple regression on dummies: lm(Y~dummies, data)

### 4. Random Variations 
(Erratic movements that are not predictable because they don’t follow a pattern. Example: strike, fire, war, flood, earthquake, etc..)


## Data Cleaning
### Missing Value Interpolation
The zoo package in R is particularly useful for handling irregular time series data, including filling in missing dates and values.

zoo_object <- zoo(data_vector, order_by = date_vector)

The zoo package offers several NA handling functions:

Linear interpolation:
na_approx(zoo_data)

Last observation carried forward:
na_locf(zoo_data)

Spline interpolation (uses piecewise polynomial functions to divide data range into intervals and fits a different polynomial to each interval to create a smooth, continuous curve_):
na_spline(zoo_data)

Aggregate by period (e_g_, monthly average):
aggregate(zoo_data, as_yearmon, mean)


### Outliers

Types of Outliers
1.Additive Outlier (AO) 
A surprisingly large or small value, affects a single observation.

2.Level Shift (LS)
LS change the level of time series, at a given time peiod all observations changed to a new level. Maybe changes in concepts or compilation methods of survey population. But seasonal behavior doesn't change.

3.Transient Change (TC)
A shorter period of level change but eventually returns to its normal level.May be deviation from average monthly weather conditions.

outliers::outlier () - find value with largest difference from the mean

outliers::rm.outlier(x, fill = FALSE, median = FALSE, opposite = 
FALSE) -Remove the value(s) most differing from the mean

outliers::dixon.test(x, type = 0, opposite = FALSE, two.sided= 
TRUE) - Dixon tests for outlier

outliers::chisq.out.test(x, variance=var(x), opposite = FALSE) Chi-squared test for outlier
 
outliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided= 
FALSE) - Grubbs tests for one or two outliers in data sample

Package reference: https://env790.github.io/docs/modules/readings/M4_RPackage-Outliers.pdf



# ARIMA Model - Parametric Time Series Model  
Parametric - Focus on distribution of the Time Series. Meaning that the model assumes that the time series follows a particular distribution (e.g., Gaussian, Poisson, Exponential). For example, in ARIMA, the errors are typically assumed to be normally distributed.

ARIMA model is one of the Traditional Box-Jenkins Models - The series has to be stationary (Constant Mean, Variance, and correlation structure).
## AR (Autoregressive)
- Regress independent variable based on previous observations (residuals should have a mean of 0) plus a random shock (error).
- AR process have infinite non-zero autocorrelation coefficients that decay with the lag, so have a relatively "long memory", what happened before will impact current value.
- Maximum Likelihood Estimation (MLE) is default to be used in AR model, Conditional Sum of Squares (CSS) can be assigned as well.
- ACF for AR models, ACF will decay exponentially with time. Meaning we have some relationship between Y and previous observations.
- PACF cuts off abruptly after lag p because the partial correlation beyond lag p is zero, so this will identify the order of the AR model. 

Example of Autoregressive Model of Order 1:
AR(1) = $Y_t = C +\phi Y_{t-1} + a_t$

c ($\beta0$) is the intercept, representing a constant baseline value when Y_t-1 = 0.
$\phi$ is regression coefficient, slope ($\beta1$), representing how much Y_t depends on Y_t-1. $|\phi|$ needs to be <1 for the process to be stationary, if =1, then the process is non-stationary, stochastic trend (unit root) exists.

a_t is the random shock (error term) at t, assumed to be independent and identically distributed i.i.d., and normally distributed with mean 0 and constant variance N(0, $\theta^2$)

## MA (Moving Average) 
- Future observations will depend on previous errors.
- MA have a "short memory"
- MA ACF and PACF appears to be the opposite of AR model. ACF will help identify the order because the process only depends on the last q shocks ( MA(1) only remembers the last shock) and PACF will decay exponentially because indirect effect of past shocks persists.

MA(1) = $Y_t = \mu + a_t - \theta a_{t-1}$
$\mu$ is the process mean and a_t are i.i.d N(0, $\theta^2$)
The Expected Value of Y_t will be $\mu$ for any t, because expected value of a_t is 0, so it's not dependent in time.

## AR v.s MA

AR (p) -current value depend on its own p-previous values
MA (q) -current deviation from mean depends on q-previous deviations

Often if the stationary series has positive autocorrelation at lag 1, AR terms work best and if it has negative autocorrelation at lag 1, MA terms work best.

```{r, AR v.s. MA}
# Set seed for reproducibility
set.seed(123)

# Generate AR(1) and MA(1) processes
n <- 100
time <- 1:n

# AR(1) with phi = 0.7
ar_process <- arima.sim(model = list(ar = 0.7), n = n)

# MA(1) with theta = 0.7
ma_process <- arima.sim(model = list(ma = 0.7), n = n)

# Combine into data frame
ts_data <- data.frame(
  Time = rep(time, 2),
  Value = c(ar_process, ma_process),
  Model = rep(c("AR(1) Process", "MA(1) Process"), each = n)
)

# Create the plot
p1 <- ggplot(ts_data, aes(x = Time, y = Value, color = Model)) +
  geom_line() +
  facet_wrap(~Model, ncol = 1, scales = "free_y") +
  labs(title = "Comparison of AR(1) and MA(1) Processes",
       subtitle = "AR models show more persistence while MA models have shorter memory",
       y = "Value") +
  theme_minimal() +
  theme(legend.position = "none")

# Compute ACFs
ar_acf <- acf(ar_process, plot = FALSE)
ma_acf <- acf(ma_process, plot = FALSE)

# Prepare ACF data
acf_data <- data.frame(
  Lag = c(ar_acf$lag, ma_acf$lag),
  ACF = c(ar_acf$acf, ma_acf$acf),
  Model = rep(c("AR(1) ACF", "MA(1) ACF"), each = length(ar_acf$lag))
)

# Create the ACF plot
p2 <- ggplot(acf_data, aes(x = Lag, y = ACF, fill = Model)) +
  geom_col(width = 0.1) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Model, ncol = 1) +
  labs(title = "Autocorrelation Function Comparison",
       subtitle = "AR models show decaying ACF while MA models cut off after q lags",
       y = "Autocorrelation") +
  theme_minimal() +
  theme(legend.position = "none")

# Compute PACFs
ar_pacf <- pacf(ar_process, plot = FALSE)
ma_pacf <- pacf(ma_process, plot = FALSE)

# Prepare PACF data
pacf_data <- data.frame(
  Lag = c(ar_pacf$lag, ma_pacf$lag),
  PACF = c(ar_pacf$acf, ma_pacf$acf),
  Model = rep(c("AR(1) PACF", "MA(1) PACF"), each = length(ar_pacf$lag))
)

# Create the PACF plot
p3 <- ggplot(pacf_data, aes(x = Lag, y = PACF, fill = Model)) +
  geom_col(width = 0.1) +
  geom_hline(yintercept = 0) +
  facet_wrap(~Model, ncol = 1) +
  labs(title = "Partial Autocorrelation Function",
       subtitle = "AR(1): Cuts off after lag 1\nMA(1): Oscillating decay",
       y = "Partial Autocorrelation") +
  theme_minimal() 
print(p1)
print(p2)
print(p3)
```

## ARIMA（Autoregressive Integrated Moving Average)
I (Integrated) 
- Differencing of the time series

Find Order for your Model - How many elements do you need to have a good representation of your time series.
p:AR
d:Differences (non-seasonal)
q:MA

# Example
Forecast **daily** demand for the month of January 2011 based on this historical data_ 

```{r Data Cleaning}

hourly_load_data <- read_excel(path= "./Sample Data/load.xlsx",col_names=TRUE) 
#hourly demand from January 2005 to December 2010
humidity_data <- read_excel(path= "./Sample Data/relative_humidity.xlsx",col_names=TRUE)
#hourly temperature from January 2005 to December 2010
temperature_data <- read_excel(path= "./Sample Data/temperature.xlsx",col_names=TRUE) 
#relative humidity from January 2005 to December 2010

```

```{r Hourly to Daily}
hourly_load_data <- hourly_load_data[,-1]

daily_load_data <- hourly_load_data %>%
  rowwise() %>%
  mutate(ave_daily_load = mean(c_across(h1:h24)))%>% #average of the 24 hours
  ungroup() %>%
  select(-starts_with("h"))%>%
  mutate( Year = year(date),
          Month = month(date), 
          Day = day(date))%>%
   na.omit()

sum(is.na(daily_load_data))

ggplot(daily_load_data, aes(x=date,y=ave_daily_load)) +
  geom_line() +
  ylab("Average Daily Load")

#Create Time Series Object
daily_load_ts <- msts(daily_load_data$ave_daily_load,
                      seasonal.periods = c(7,365.25),
                      start = c(2005,1,1),
                      end = c(2010,12,31)) 
sum(is.na(daily_load_ts))

daily_load_train_ts <- subset(daily_load_ts,
                                   end = length(daily_load_ts)-365) #Jan 1st 2005 to Dec 31st 2009
daily_load_test_ts <- subset(daily_load_ts,
                                   start = length(daily_load_ts)-365) # Jan 1st 2010 to Dec 31st 2010

autoplot(daily_load_ts)
autoplot(daily_load_train_ts)
autoplot(daily_load_test_ts)

```

```{r Decomposing Time Series Objects}
#Seperating trend, cycle, seasonal, and random components.
decompose_daily_load_train_ts <- mstl(daily_load_train_ts)
autoplot(decompose_daily_load_train_ts)

```

```{r ACF,PACF}
# Option 1: The function acf() and pacf() is from package "stats"
HP1_acf=acf(daily_load_train_ts,lag.max=60, type="correlation", plot=TRUE)

HP1_pacf=pacf(daily_load_train_ts,lag.max=60, plot=TRUE)

#Option 2: Computing ACF and PACF with package "forecast"

#The next line will output a plot since we have plot=TRUE and also store the acf values in the object
HP1_acf=Acf(daily_load_train_ts,lag.max=60, type="correlation", plot=TRUE)

HP1_pacf=Pacf(daily_load_train_ts,lag.max=60, plot=TRUE)
```

```{r Fit ARIMA Model}



```

### Exponential Smoothing Space Model
```{r ETS Model}
#Forecast will be obtained by applying a non-seasonal exponential smoothing model to the seasonally adjusted data and re-seasonalizing using the last year of the seasonal component (for all seasonal components)_ The model used with `stlf` is a ETS(_,_,_) model_ ETS stands for **E**rror, **T**rend, **S**easonal and can also be be thought of as **E**xponen**T**ial **S**moothing_ 

ETS_fc <-  stlf(daily_load_train_ts, h=365)

autoplot(ETS_fc)

autoplot(daily_load_test_ts, color = "grey") +
  autolayer(ETS_fc, series="STL + ETS", color = "blue") +
  ylab("Daily Load")


```

### ARIMA with dynamic harmonic fourier components
```{r ARIMA with dynamic harmonic fourier components}

ARIMA <- auto.arima(daily_load_train_ts,
                  seasonal=TRUE, 
                  lambda=0,
         xreg=fourier(daily_load_train_ts, K=c(2,12)))

ARIMA_fc <- forecast(ARIMA,
         xreg=fourier(daily_load_train_ts, K=c(2,12),h=365),h=365)

#Plot forecast with observed data
ARIMA_plot <- autoplot(daily_load_test_ts, color = "grey") +
  autolayer(ARIMA_fc$mean, series="ARIMA_xgre",color = "blue") +
  ylab("Daily Load")
ARIMA_plot

```

### TBATS
```{r TBATS}

TBATS_fit <- tbats(daily_load_train_ts)

#Forecast with TBATS
TBATS_fc <- forecast(TBATS_fit, h=365)

#Plot forecast with observed data
autoplot(daily_load_test_ts, color = "grey") +
  autolayer(TBATS_fc$mean, series="TBATS", color = "blue")+
  ylab("Daily Load")
```

### Neural Network
```{r Neural Network}
NN_fit <- nnetar(daily_load_train_ts,p=1,P=0,xreg=fourier(daily_load_train_ts, K=c(2,12)))

#Forecast with NNet
NN_fc<- forecast(NN_fit,h=365,xreg=fourier(daily_load_train_ts,K=c(2,12),h=365))

#Plot forecast with observed data
autoplot(daily_load_test_ts, color = "dark grey") +
  autolayer(NN_fc$mean, series="Neural Network",color = "blue")+
  ylab("Electricity Price") 


```

### Residual Diagnostics
Ljung-Box test: A p-value > 0.05 suggests residuals are uncorrelated.

ACF/PACF plots: Look for significant lags.

```{r Residual Diagnostics}
checkresiduals(ARIMA_fc)
```

### Checking accuracy of models

```{r Checking Accuracy}

# print(time(ETS_fc$mean))
# print(time(daily_load_test_ts))

#Model 1: STL + ETS
ETS_score <- accuracy(ETS_fc$mean, daily_load_test_ts)  
#Model 2: ARIMA 
ARIMA_score <- accuracy(ARIMA_fc$mean, daily_load_test_ts)

# Model 3:  TBATS 
TBATS_score <- accuracy(TBATS_fc$mean, daily_load_test_ts)
# Model 4:  Neural Network 
NN_score <- accuracy(NN_fc$mean, daily_load_test_ts)

scores <- as.data.frame(rbind(ETS_score, ARIMA_score, TBATS_score, NN_score))
  
row.names(scores) <- c("ETS", "ARIMA","TBATS", "NN")


#choose model with lowest RMSE
best_model_index <- which.min(scores[,"RMSE"])

cat("The best model by RMSE is:", row.names(scores[best_model_index,]))    

kbl(scores, 
      caption = "Forecast Accuracy for Electricity Price",
      digits = array(5,ncol(scores))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which_min(scores[,"RMSE"]))


autoplot(daily_load_test_ts) +
  autolayer(ETS_fc, PI=FALSE, series="STL+ETS") +
  autolayer(ARIMA_fc, PI=FALSE, series="ARIMA") +
  autolayer(TBATS_fc,PI=FALSE, series="TBATS") +
  autolayer(NN_fc,PI=FALSE, series="NN") +
  xlab("Day") + ylab("Daily Load Forecast") +
  guides(colour=guide_legend(title="Forecast"))

```

### Predicting the future

```{r}

#TBATS
TBATS <- tbats(daily_load_ts)
TBATS_fc <- forecast(TBATS, h=365)
autoplot(daily_load_ts, color = "grey") +
  autolayer(TBATS_fc$mean, series="TBATS", color = "blue")+
  ylab("Load Forecast")


# #NNet
# NN <- nnetar(daily_load_ts,p=1,P=0,xreg=fourier(daily_load_ts, K=c(2,12)))
# NN_fc<- forecast(NN,h=365,xreg=fourier(daily_load_train_ts,K=c(2,12),h=365))
# autoplot(daily_load_ts, color = "dark grey") +
#   autolayer(NN_fc$mean, series="NN+Fourier", color = "blue") +
#   ylab("Load Forecast")

```